{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation of CCIC against HAIC-HIWC IKP-2 measurements\n",
    "\n",
    "This notebook compiles the code required to collocate and analyse the IKP-2 measurements during the HAIC-HIWC campaign.\n",
    "\n",
    "Files containing the IKP-2 measurements are prepared from the raw format given by the data provider into a suitable format elsewhere.\n",
    "\n",
    "The notebook uses `if False:` statements to comment out code needs to be run only once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import datetime\n",
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import tqdm\n",
    "import xarray as xr\n",
    "\n",
    "from ccic.data.gridsat import GridSat\n",
    "from ccic.data.cpcir import CPCIR\n",
    "from ccic.plotting import set_style\n",
    "from ccic.validation import get_latlon_bins, resample_data\n",
    "\n",
    "# Set plotting style\n",
    "set_style()\n",
    "\n",
    "# Set Cartopy directory\n",
    "os.environ[\"CARTOPY_USER_BACKGROUNDS\"] = '/mnt/data_sun/ccic/misc'\n",
    "Image.MAX_IMAGE_PIXELS = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Links to the folders containing the IKP-2 data files\n",
    "orders = [\n",
    "    'amell60263', # 2014, https://doi.org/10.5065/D6WW7GDS\n",
    "    'amell60877', # 2015, https://doi.org/10.5065/D61N7ZV7\n",
    "    'amell61210', # 2015, https://doi.org/10.5065/D6RN36KJ\n",
    "    'amell32226'  # 2018, https://doi.org/10.26023/8V5Y-GB2E-CX07\n",
    "]\n",
    "\n",
    "orders_path = Path('/mnt/data_sun/ccic/flight_campaigns/HAICHIWC/orders/friendly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the spatial and temporal boundaries of each flight\n",
    "# Also keep the paths of the flights\n",
    "flight_tracks = []\n",
    "timestamps_flight_boundaries = []\n",
    "roi_flight_boundaries = []\n",
    "roi_campaign = [] # Record by campaign as well\n",
    "for i, order in enumerate(orders):\n",
    "    files_order = sorted(glob.glob(str(orders_path / order / '*csv')))\n",
    "    flight_tracks_order = []\n",
    "    _roi_campaign = []\n",
    "    for f in files_order:\n",
    "        df = pd.read_csv(f)\n",
    "        # Filter only measurements where TWC has a valid (non-negative) value\n",
    "        df_filter = df[(pd.isna(df.TWC_gm3) == False) & (df.TWC_gm3 >= 0)].reset_index(drop=True)\n",
    "        # Format time\n",
    "        df_filter['UTC'] = pd.to_datetime(df_filter.UTC)\n",
    "        # Record the the flight tracks and boundaries\n",
    "        flight_tracks_order.append(df_filter[['longitude', 'latitude']].values)\n",
    "        timestamps_flight_boundaries.append([df_filter.UTC.min(), df_filter.UTC.max()])\n",
    "        roi_flight_boundaries.append([df_filter.longitude.min(), df_filter.latitude.min(), df_filter.longitude.max(), df_filter.latitude.max()])\n",
    "        _roi_campaign.append(roi_flight_boundaries[-1])\n",
    "    roi_campaign.append(_roi_campaign)\n",
    "    flight_tracks.append(flight_tracks_order)\n",
    "\n",
    "# Having a ROI for each campaign will be more convenient later on\n",
    "roi_campaign_flight = []\n",
    "for c in roi_campaign:\n",
    "    for _ in c:\n",
    "        roi_campaign_flight.append(np.array(c).min(axis=0)[:2].tolist() + np.array(c).max(axis=0)[2:].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check: plot the flight tracks\n",
    "\n",
    "# Create a figure and axis with the Robinson projection\n",
    "fig, ax = plt.subplots(subplot_kw=dict(projection=ccrs.Robinson()))\n",
    "\n",
    "# Set the extent to global\n",
    "ax.set_global()\n",
    "\n",
    "ax.add_feature(cfeature.COASTLINE)\n",
    "\n",
    "for campaign_i in range(4):\n",
    "    for track in flight_tracks[campaign_i]:\n",
    "        ax.scatter(track[:,0], track[:,1], c=f'C{campaign_i}', s=1, transform=ccrs.PlateCarree())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list with the file names to download\n",
    "files_gridsat_to_download = []\n",
    "files_cpcir_to_download = []\n",
    "for (t_start, t_end)  in timestamps_flight_boundaries:\n",
    "    for t in pd.date_range(t_start.floor('H'), t_end.ceil('H'), freq='H', inclusive='both'):\n",
    "        files_gridsat_to_download.append(t.strftime('GRIDSAT-B1.%Y.%m.%d.%H.v02r01.nc'))\n",
    "        files_cpcir_to_download.append(t.strftime('merg_%Y%m%d%H_4km-pixel.nc4'))\n",
    "\n",
    "# Gridsat files only exist every three hours, therefore clear list from files that will not exist\n",
    "files_gridsat_to_download = set.intersection(\n",
    "    set(files_gridsat_to_download),\n",
    "    set([t.strftime('GRIDSAT-B1.%Y.%m.%d.%H.v02r01.nc') for t in pd.date_range(pd.Timestamp('2014'), pd.Timestamp('2019'), freq='3H', inclusive='both')])\n",
    ")\n",
    "files_cpcir_to_download = set(files_cpcir_to_download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the data\n",
    "INPUT_DATA_PATH = Path('/data/ccic_input_haichiwc_ikp2')\n",
    "\n",
    "if False:\n",
    "    for f in tqdm.tqdm(files_gridsat_to_download, ncols=80):\n",
    "        GridSat.download(f, str(INPUT_DATA_PATH / 'gridsat' / f))\n",
    "\n",
    "    for f in tqdm.tqdm(files_cpcir_to_download, ncols=80):\n",
    "        CPCIR.download(f, str(INPUT_DATA_PATH / 'cpcir' / f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the commands to run CCIC retrievals.\n",
    "\n",
    "We need retrievals $\\pm 15$ minutes from the IKP-2 measurement, if the product allows for it. In addition, we'll only do inference for the ROI covered by the flight track. We'll use a generous ROI to stay away from any potential edge effect.\n",
    "\n",
    "Note: at the time of writing this notebook, the `ccic process` command accepts the following arguments:\n",
    "\n",
    "```{toggle}\n",
    "$ ccic process --help\n",
    "usage: ccic process [-h] [--input_path path] [--targets target [target ...]] [--tile_size N] [--overlap N] [--device dev] [--precision 16/32]\n",
    "                    [--output_format netcdf/zarr] [--database_path path] [--roi x x x x] [--n_processes n] [--inpainted_mask]\n",
    "                    [--confidence_interval CONFIDENCE_INTERVAL]\n",
    "                    model cpcir/gridsat output t1 [t2]\n",
    "\n",
    "Run CCIC retrieval.\n",
    "\n",
    "positional arguments:\n",
    "  model                 Path to the trained CCIC model.\n",
    "  cpcir/gridsat         For which type of input to run the retrieval.\n",
    "  output                Folder to which to write the output.\n",
    "  t1                    The time for which to run the retrieval.\n",
    "  t2                    If given, the retrieval will be run for all files in the time range t1 <= t <= t2\n",
    "\n",
    "options:\n",
    "  -h, --help            show this help message and exit\n",
    "  --input_path path     Path to a local directory containing input files. If not given, input files will be downloaded using pansat.\n",
    "  --targets target [target ...]\n",
    "  --tile_size N         Tile size to use for processing.\n",
    "  --overlap N           Tile size to use for processing.\n",
    "  --device dev          The name of the torch device to use for processing.\n",
    "  --precision 16/32     The precision with which to run the retrieval. Only has an effect for GPU processing.\n",
    "  --output_format netcdf/zarr\n",
    "                        The output format in which to store the output: 'zarr' or 'netcdf'. 'zarr' format applies a custom filter to allow storing 'tiwp' fields as\n",
    "                        8-bit integer, which significantly reduces the size of the output files.\n",
    "  --database_path path  Path to the database to use to log processing progress.\n",
    "  --roi x x x x\n",
    "  --n_processes n\n",
    "  --inpainted_mask      Create a variable `inpainted` indicating if the retrieved pixel is inpainted (the input data was NaN).\n",
    "  --confidence_interval CONFIDENCE_INTERVAL\n",
    "                        Width of the equal-tailed confidence interval to use to report retrieval uncertainty of scalar retrieval targets. Must be within [0, 1].\n",
    "```\n",
    "\n",
    "We'll use a GPU from the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some constants matching how the container will be executed\n",
    "model = '/data/ccic.pckl'       # CCIC model to use\n",
    "input_path = Path('/data/input')\n",
    "output_path = Path('/data/output')   # Where to store the output\n",
    "delta_roi = 2.5                 # Expand the region covered by the flight track with this many degrees\n",
    "commands = []                   # List to store shell commands to run the retrieval\n",
    "\n",
    "# Build the commands\n",
    "# We will use the ROI for each campaign rather than the ROI for each flight,\n",
    "# which avoids problem later when creating the collocations\n",
    "for ((time_start, time_end), roi) in zip(timestamps_flight_boundaries, roi_campaign_flight):\n",
    "    # Add margins to ROI\n",
    "    roi[0] = roi[0] - delta_roi\n",
    "    roi[1] = roi[1] - delta_roi\n",
    "    roi[2] = roi[2] + delta_roi\n",
    "    roi[3] = roi[3] + delta_roi\n",
    "\n",
    "    roi_str = ' '.join([str(e) for e in roi])\n",
    "    \n",
    "    # We'll use the .floor and .ceil methods of pd.Timestamp for the +/- 15 min requirement\n",
    "    args_cpcir = [model, 'cpcir', output_path / 'cpcir',\n",
    "                  time_start.floor('H').strftime('%Y-%m-%dT%H:%M'),\n",
    "                  time_end.ceil('H').strftime('%Y-%m-%dT%H:%M'),\n",
    "                  roi_str,\n",
    "                  input_path / 'cpcir']\n",
    "    args_gridsat = [model, 'gridsat', output_path / 'gridsat',\n",
    "                    time_start.floor('3H').strftime('%Y-%m-%dT%H:%M'), # 3-hour resolution\n",
    "                    time_end.ceil('3H').strftime('%Y-%m-%dT%H:%M'), # 3-hour resolution\n",
    "                    roi_str,\n",
    "                    input_path / 'gridsat']\n",
    "    commands.append('ccic process {:} {:} {:} {:} {:} --roi {:} --input_path {:} --targets tiwc --device cuda'.format(*args_cpcir))\n",
    "    commands.append('ccic process {:} {:} {:} {:} {:} --roi {:} --input_path {:} --targets tiwc --device cuda'.format(*args_gridsat))\n",
    "\n",
    "# Save these commands into a shell script and execute in the cluster for speed\n",
    "# The shell script is adapted for the cluster outside of this notebook\n",
    "if False:\n",
    "    with open('validation_haic_hiwc_ikp2.sh', 'w') as handle:\n",
    "        handle.write('\\n'.join(commands))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll do a sanity check: the retrievals produced cover the flight tracks in space and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filename_to_ts(f):\n",
    "    \"\"\"Extract the timestamp from the filenames\"\"\"\n",
    "    return datetime.datetime.strptime(Path(f).name.split('_')[-1], '%Y%m%d%H00.nc')\n",
    "\n",
    "output_files = {\n",
    "    'cpcir': {filename_to_ts(f): f for f in sorted(glob.glob('/data/ccic_output_haichiwc_ikp2/cpcir/*nc'))},\n",
    "    'gridsat': {filename_to_ts(f): f for f in sorted(glob.glob('/data/ccic_output_haichiwc_ikp2/gridsat/*nc'))}\n",
    "}\n",
    "\n",
    "# Re-structure the dict and apply time formatting\n",
    "for product, file_dict in output_files.items():\n",
    "    output_files[product] = [\n",
    "        np.array(list(file_dict.keys()), dtype='datetime64'),\n",
    "        np.array(list(file_dict.values()))\n",
    "    ]\n",
    "\n",
    "for i, ((t_start, t_end), (lon_min, lat_min, lon_max, lat_max)) in tqdm.tqdm(enumerate(zip(timestamps_flight_boundaries, roi_flight_boundaries)), total=len(timestamps_flight_boundaries)):\n",
    "    product = 'cpcir'\n",
    "    unit = '1H'\n",
    "    times_ir, files_ir = output_files[product]\n",
    "    idx_times = (t_start.floor(unit) <= times_ir) & (times_ir <= t_end.ceil(unit))\n",
    "    assert idx_times.any()\n",
    "    retrievals_flight = files_ir[idx_times]\n",
    "    lon_min_ds, lat_min_ds, lon_max_ds, lat_max_ds = [], [], [], []\n",
    "    for f in retrievals_flight:\n",
    "        ds = xr.open_dataset(f)\n",
    "        lon_min_ds.append(ds.longitude.values.min())\n",
    "        lat_min_ds.append(ds.latitude.values.min())\n",
    "        lon_max_ds.append(ds.longitude.values.max())\n",
    "        lat_max_ds.append(ds.latitude.values.max())\n",
    "        assert min(lon_min_ds) <= lon_min <= max(lon_max_ds)\n",
    "        assert min(lat_min_ds) <= lat_min <= max(lat_max_ds)\n",
    "\n",
    "        # This is to check that the ROIs for the retrieval are all the same\n",
    "        assert np.unique(lon_min_ds).size == 1\n",
    "        assert np.unique(lat_min_ds).size == 1\n",
    "        assert np.unique(lon_max_ds).size == 1\n",
    "        assert np.unique(lat_max_ds).size == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No AssertionError was raised, which means that all flights have retrievals. Next, we'll resample the flight data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_csv(f):\n",
    "    # Read CSV\n",
    "    df = pd.read_csv(f, parse_dates=['UTC'])[['UTC', 'latitude', 'longitude', 'altitude_pressure_metres', 'TWC_gm3']]\n",
    "    # Keep only valid TWC measurements\n",
    "    df = df[(pd.isna(df.TWC_gm3) == False) & (df.TWC_gm3 >= 0)].reset_index(drop=True)\n",
    "    # Rename\n",
    "    df = df.rename(columns={'UTC': 'time', 'altitude_pressure_metres': 'altitude', 'TWC_gm3': 'twc'})\n",
    "    # Set time as index\n",
    "    df = df.set_index('time')\n",
    "    return xr.Dataset(df)\n",
    "\n",
    "RETRIEVALS_PATH = Path('/data/ccic_output_haichiwc_ikp2')\n",
    "RESAMPLE_OUTPUT_PATH = Path('/data/ccic_haichiwc_ikp2_resampled')\n",
    "\n",
    "if False:\n",
    "    flight_count = 0\n",
    "    for order in orders:\n",
    "        files_order = sorted(glob.glob(str(orders_path / order / '*csv')))\n",
    "        for f in files_order:\n",
    "            ds = prepare_csv(f)\n",
    "            ds_t_start = pd.to_datetime(ds.time.values[0])\n",
    "            for product, freq in {'cpcir': '1H', 'gridsat': '3H'}.items():\n",
    "                # Open a retrieval sample just to get the grid\n",
    "                retrieval_sample = f\"ccic_{product}_{ds_t_start.ceil(freq).strftime('%Y%m%d%H')}00.nc\"\n",
    "                bin_lats, bin_lons = get_latlon_bins(RETRIEVALS_PATH / product / retrieval_sample)\n",
    "                # Resample\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.filterwarnings('ignore', 'Index.ravel returning ndarray')\n",
    "                    # Note: the current version of resample_data can produce files that do not have\n",
    "                    #       any variable. Example: measurements start at T = 20:59. The 20 h file\n",
    "                    #       will be produced with no variables, but there are no collocations with it\n",
    "                    resample_data(ds, ['twc'], bin_lons, bin_lats, RESAMPLE_OUTPUT_PATH / product / order,\n",
    "                                f\"in_situ_{product}_{{year:04}}{{month:02}}{{day:02}}{{hour:02}}.nc\")\n",
    "            flight_count += 1\n",
    "            print(f\"Processed {flight_count} flights\", end='\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collocate the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RETRIEVALS_IKP2_PATH = Path('/data/ccic_haichiwc_ikp2_resampled_and_collocated')\n",
    "\n",
    "if False:\n",
    "    with tqdm.tqdm(total=len(list((RESAMPLE_OUTPUT_PATH).glob('**/*.nc')))) as pbar:\n",
    "        for order in orders:\n",
    "            for product, freq in {'cpcir': 1, 'gridsat': 3}.items():\n",
    "                resampled_files = list((RESAMPLE_OUTPUT_PATH / product / order).glob('*.nc'))\n",
    "                for f in resampled_files:\n",
    "                    pbar.update()\n",
    "                    ts = pd.to_datetime(datetime.datetime.strptime(f.name.split('_')[-1], '%Y%m%d%H.nc'))\n",
    "                    if (ts.hour % freq) != 0:\n",
    "                        # Deal with the 3-hour resolution of GridSat\n",
    "                        continue\n",
    "                    ds_resampled = xr.open_dataset(f)\n",
    "                    retrieval_base_fname = f'ccic_{product}_{ts.strftime(\"%Y%m%d%H\")}'\n",
    "                    ds_retrieval = xr.open_dataset(RETRIEVALS_PATH / product / f'{retrieval_base_fname}00.nc')\n",
    "\n",
    "                    # Sanity checks:\n",
    "                    assert (abs(ds_resampled.latitude - ds_retrieval.latitude) <= 0.001).all()\n",
    "                    assert (abs(ds_resampled.longitude - ds_retrieval.longitude) <= 0.001).all()\n",
    "                    assert np.allclose(ds_resampled.altitude * 1e3, ds_retrieval.altitude)\n",
    "\n",
    "                    # Add in-situ tiwp to retrieval\n",
    "                    if 'twc' not in ds_resampled:\n",
    "                        # This is possible, for an explanation see the previous code cell\n",
    "                        continue\n",
    "                    # Filter by time: will only have an effect for GridSat\n",
    "                    #                 also need to handle floating point issues with datetime64...\n",
    "                    idx_time = ds_resampled.time.values.astype('datetime64[m]') == ds_retrieval.time.values.astype('datetime64[m]')\n",
    "                    ds_resampled = ds_resampled.sel(time=idx_time)\n",
    "                    \n",
    "                    # Sanity checks\n",
    "                    assert (abs(ds_resampled.longitude.values - ds_retrieval.longitude.values) < 0.001).all()\n",
    "                    assert (abs(ds_resampled.latitude.values - ds_retrieval.latitude.values) < 0.001).all()\n",
    "                    \n",
    "                    dims = list(ds_retrieval.dims)\n",
    "                    ds_retrieval[\"twc_ikp2\"] = (dims, ds_resampled.transpose(*dims).twc.data)\n",
    "\n",
    "                    # Write to disk\n",
    "                    ds_retrieval.to_netcdf(RETRIEVALS_IKP2_PATH / product / order / f'{retrieval_base_fname}_ikp2.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we construct arrays containing all valid retrieval-measurement collocations and locations, sorted by campaign and product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    files_processed = 0\n",
    "    data = {order: {'cpcir': None, 'gridsat': None} for order in orders}\n",
    "    for order, collocations in data.items():\n",
    "        for product in collocations.keys():\n",
    "            files = Path(RETRIEVALS_IKP2_PATH / product / order).glob('*nc')\n",
    "            data_campaign = []\n",
    "            for f in files:\n",
    "                ds = xr.open_dataset(f).transpose('longitude', 'latitude', 'altitude', 'time')\n",
    "                valid = np.isfinite(ds.tiwc) * (ds.tiwc >= 0) * np.isfinite(ds.twc_ikp2) * (ds.twc_ikp2 >= 0)\n",
    "                valid_idx = np.nonzero(valid.values)\n",
    "                longitudes = ds.longitude.values[valid_idx[0]]\n",
    "                latitudes = ds.latitude.values[valid_idx[1]]\n",
    "                tiwc = ds.tiwc.values[valid]\n",
    "                twc_ikp2 = ds.twc_ikp2.values[valid]\n",
    "                data_campaign.append(np.array([longitudes, latitudes, tiwc, twc_ikp2]).T)\n",
    "                files_processed += 1\n",
    "                print(f'Files processed: {files_processed}', end='\\r')\n",
    "            pd.DataFrame(np.concatenate(data_campaign, axis=0), columns=['longitude', 'latitude', 'tiwc', 'twc_ikp2']).to_xarray()\n",
    "            data[order][product] = pd.DataFrame(np.concatenate(data_campaign, axis=0), columns=['longitude', 'latitude', 'tiwc', 'twc_ikp2']).to_xarray()\n",
    "    with open('/mnt/data_sun/ccic/analyses/haic-hiwc/collocated.pickle', 'wb') as handle:\n",
    "        pickle.dump(data, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First load the pre-computed data\n",
    "with open('/mnt/data_sun/ccic/analyses/haic-hiwc/collocated.pickle', 'rb') as handle:\n",
    "    data = pickle.load(handle)\n",
    "\n",
    "# Re-organize dictionary keys\n",
    "orders_map = {\n",
    "    'amell60263': 0, # 2014, https://doi.org/10.5065/D6WW7GDS\n",
    "    'amell60877': 1, # 2015, https://doi.org/10.5065/D61N7ZV7\n",
    "    'amell61210': 2, # 2015, https://doi.org/10.5065/D6RN36KJ\n",
    "    'amell32226': 3  # 2018, https://doi.org/10.26023/8V5Y-GB2E-CX07\n",
    "}\n",
    "\n",
    "data = {v: data[k] for k, v in orders_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(frameon=False)\n",
    "\n",
    "axs = [\n",
    "    fig.add_axes([0, 0, 1, 1], projection=ccrs.PlateCarree()),\n",
    "    fig.add_axes([0.33, 0.175, 0.35, 0.4], projection=ccrs.PlateCarree())\n",
    "]\n",
    "\n",
    "colors = {0: 'C1', 1: 'C0', 2: 'C3', 3: 'w'}\n",
    "labels = {\n",
    "    0: 'Jan--Feb 2014', 1: 'May 2015', 2: 'Aug 2015', 3: 'Jul--Aug 2018'\n",
    "}\n",
    "\n",
    "\n",
    "for campaign_idx, campaign_data in data.items():\n",
    "    if campaign_idx == 0:\n",
    "        ax = axs[1]\n",
    "    else:\n",
    "        ax = axs[0]\n",
    "\n",
    "    for ds in campaign_data.values():\n",
    "        ax.scatter(ds.longitude, ds.latitude, s=.5, c=colors[campaign_idx], transform=ccrs.PlateCarree(), rasterized=True)\n",
    "\n",
    "for ax in axs:\n",
    "    ax.background_img(name='blue_marble_dec', resolution='low')\n",
    "\n",
    "axs[0].gridlines(crs=ccrs.PlateCarree(), draw_labels={\"top\": \"x\", \"left\": \"y\"}, linestyle='dotted', alpha=0.3)\n",
    "axs[1].gridlines(crs=ccrs.PlateCarree(), draw_labels={\"bottom\": \"x\", \"right\": \"y\"}, ylabel_style={'color': 'white'}, linestyle='dotted', alpha=0.3)\n",
    "\n",
    "axs[0].set_extent([-160, -45, 0, 40], crs=ccrs.PlateCarree())\n",
    "axs[1].set_extent([120, 145, -20, -7.5], crs=ccrs.PlateCarree())\n",
    "\n",
    "legend_elements = [matplotlib.lines.Line2D([0], [0], lw=0, color=colors[i], label=labels[i], markerfacecolor=colors[i], marker='s') for i in range(4)]\n",
    "\n",
    "axs[0].legend(handles=legend_elements, loc='upper left', facecolor='lightgray', handlelength=0.5)\n",
    "\n",
    "fig.savefig('haic_hiwc_coverage.pdf', bbox_inches='tight')\n",
    "\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [0] + np.logspace(-4, 1, 55, endpoint=False).tolist() + [10]\n",
    "\n",
    "norm = matplotlib.colors.LogNorm(1e-2, 1e2)\n",
    "txtcol = 'C0'\n",
    "diagcol = 'orangered'\n",
    "\n",
    "scale_figsize = 1.25\n",
    "fig, axs = plt.subplots(ncols=2, nrows=4, figsize=(4*scale_figsize,8*scale_figsize), sharex=True, sharey=True)\n",
    "\n",
    "label_i = 0\n",
    "for campaign_idx, campaign_data in data.items():\n",
    "    for product_idx, product in enumerate(['CPCIR', 'GridSat']):\n",
    "        data_plot = campaign_data[product.lower()]\n",
    "        ax = axs[campaign_idx,product_idx]\n",
    "\n",
    "        # Keep only valid values\n",
    "        tiwc = data_plot.tiwc.values\n",
    "        twc = data_plot.twc_ikp2.values\n",
    "        idxs = np.isfinite(twc) & np.isfinite(tiwc) # This should be redundant with a pre-processing step\n",
    "        twc = twc[idxs]\n",
    "        tiwc = tiwc[idxs]\n",
    "\n",
    "        H, _, _ = np.histogram2d(twc, tiwc, bins=bins, density=True)\n",
    "\n",
    "        normalization = np.tile(np.sum(H * np.diff(bins)[None], axis=1, keepdims=True), (1, H.shape[1]))\n",
    "        H = np.divide(H, normalization, out=np.full_like(H, np.nan), where=normalization > 0)\n",
    "\n",
    "        mesh = ax.pcolormesh(bins[1:-1], bins[1:-1], H[1:-1, 1:-1].T, norm=norm, rasterized=True)\n",
    "\n",
    "        corr = np.corrcoef(twc, tiwc)[0, 1]\n",
    "        bias = (tiwc - twc).mean() / twc.mean()\n",
    "        props = dict(facecolor='white', alpha=1.0, edgecolor=\"k\")\n",
    "        # ax.text(1e-1, 1.5e-3, f\"Corr.: {corr:0.2f} \\n Bias: {100 * bias:0.2f}\\%\",\n",
    "        #         fontsize=12, color=txtcol, ha=\"left\", va=\"bottom\", bbox=props)\n",
    "        stats_pos_x = 2e-1\n",
    "        stats_string = f\"{corr:0.2f}\\n{100 * bias:0.2f}\\%\"\n",
    "        ax.text(stats_pos_x, 3e-4, stats_string,\n",
    "                fontsize=10, color=txtcol, ha=\"left\", va=\"bottom\", bbox=props)\n",
    "        # ax.set_title(f'({chr(97 + (i + i_campaign-1))}) {product}', fontsize=14)\n",
    "        # pos_text_labels = (2.5e-1, 1.5e-3)\n",
    "        pos_text_labels = (1.25e-4, 2.5)\n",
    "        ax.text(*pos_text_labels, f'({chr(97 + label_i)})', fontsize=14)\n",
    "        label_i += 1\n",
    "\n",
    "\n",
    "for ax in axs.ravel():\n",
    "    ax.set_xlim(1e-4, 1e1)\n",
    "    ax.set_ylim(1e-4, 1e1)\n",
    "    ax.plot(bins, bins, c=\"grey\", ls=\"--\")\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "# for ax in axs[:,0]:\n",
    "#     ax.set_ylabel(r'CCIC TIWC [$\\si{\\gram\\per\\cubic\\meter}$]')\n",
    "axs[1,0].set_ylabel(r'CCIC TIWC [$\\si{\\gram\\per\\cubic\\meter}$]')\n",
    "# for ax in axs[-1,:]:\n",
    "#     ax.set_xlabel(r'HAIC-HIWC TWC [$\\si{\\gram\\per\\cubic\\meter}$]')\n",
    "fig.text(0.5, 0.06, r'HAIC-HIWC TWC [$\\si{\\gram\\per\\cubic\\meter}$]', ha='center')\n",
    "\n",
    "for i, ax in enumerate(axs[:,-1]):\n",
    "    ax.yaxis.set_label_position(\"right\")\n",
    "    ax.set_ylabel(labels[i], rotation=-90, labelpad=15)\n",
    "\n",
    "axs[0,0].set_title('CPCIR', loc='center')\n",
    "axs[0,1].set_title('GridSat', loc='center')\n",
    "\n",
    "#     fig.colorbar(mesh, cax=axs[2], label=r'p(IWC $|$ TWC) [$\\si{\\per\\gram\\cubic\\meter}$]', extend='both')\n",
    "\n",
    "ax_cbar = fig.add_axes([0,0,1,0.05])\n",
    "ax_cbar.set_axis_off()\n",
    "cbar = fig.colorbar(mesh, ax=ax_cbar, label=r'p(TIWC $|$ TWC) [$\\si{\\per\\gram\\cubic\\meter}$]', extend='both', orientation='horizontal', fraction=0.4)\n",
    "cbar.ax.xaxis.set_label_position('top')\n",
    "\n",
    "fig.subplots_adjust(hspace=0.075, wspace=0.025)\n",
    "\n",
    "fig.savefig(f'haic-hiwc_tiwc_vs_twc_campaigns_individual.pdf', bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ccic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
